{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddd51d",
   "metadata": {},
   "source": [
    "# Classification and Regression Tree (CART): Decision Tree\n",
    "\n",
    "## Introduction to Decision Trees\n",
    "Decision Trees are supervised learning models that split data into regions by asking a sequence of if/else questions on features. Each internal node applies a rule (e.g., petal length ≤ 2.45), and leaves hold final predictions (a class label for classification or a value for regression).\n",
    "\n",
    "- Greedy construction: At each node, choose the best split to reduce impurity (classification) or variance (regression).\n",
    "- Advantages: Interpretable, handles mixed feature types, little preprocessing needed, captures nonlinearities and interactions.\n",
    "- Limitations: Prone to overfitting without constraints; unstable to small data changes; axis-aligned splits only.\n",
    "\n",
    "## Interpreting a Decision Tree\n",
    "- Root: The first, most informative split in the dataset.\n",
    "- Internal node: A rule “feature ≤ threshold”. Left branch is True; right branch is False.\n",
    "- Leaf: Final decision. \n",
    "  - Classification: distribution of classes, predicted class, impurity (Gini/Entropy), samples.\n",
    "  - Regression: predicted value and number of samples.\n",
    "- Depth: Path length from root to leaf. Shallower trees generalize better; deeper trees may overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbff072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual: A simple 1D split demonstrating a tree partition\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Synthetic 1D data and labels by a threshold rule\n",
    "np.random.seed(42)\n",
    "X1 = np.linspace(0, 10, 80)\n",
    "y1 = (X1 > 5).astype(int)  # true rule: class 0 for <=5, class 1 for >5\n",
    "\n",
    "# Add noise to show imperfect separability\n",
    "X1_noisy = X1 + np.random.normal(0, 0.5, size=X1.shape)\n",
    "\n",
    "# Evaluate impurity (Gini) for candidate thresholds\n",
    "thresholds = np.linspace(1, 9, 17)\n",
    "\n",
    "def gini_of_split(X, y, t):\n",
    "    left = y[X <= t]\n",
    "    right = y[X > t]\n",
    "    def gini(group):\n",
    "        if len(group) == 0: return 0.0\n",
    "        p = np.mean(group)\n",
    "        return 2 * p * (1 - p)\n",
    "    n = len(y)\n",
    "    return (len(left)/n) * gini(left) + (len(right)/n) * gini(right)\n",
    "\n",
    "scores = [gini_of_split(X1_noisy, y1, t) for t in thresholds]\n",
    "best_t = thresholds[np.argmin(scores)]\n",
    "\n",
    "print(f\"Best threshold by Gini: {best_t:.2f} (lower is better)\")\n",
    "\n",
    "# Visualize split\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.scatter(X1_noisy, y1, c=y1, cmap='coolwarm', s=40, alpha=0.8)\n",
    "plt.axvline(best_t, color='black', linestyle='--', label=f'Threshold = {best_t:.2f}')\n",
    "plt.yticks([0,1], ['Class 0','Class 1'])\n",
    "plt.xlabel('Feature X1 (noisy)')\n",
    "plt.title('Conceptual 1D Split (Gini-optimal Threshold)')\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classification in Python (Iris)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=3,      # limit depth for interpretability\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Iris Decision Tree')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Visualize tree\n",
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(clf, feature_names=feature_names, class_names=class_names,\n",
    "          filled=True, rounded=True, impurity=True)\n",
    "plt.title('Decision Tree (Iris, depth=3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62aa40c",
   "metadata": {},
   "source": [
    "### Interpreting the Decision Tree Classification Output\n",
    "- We train a CART classifier (Gini) with max_depth=3 on the Iris dataset and report accuracy and a full classification report.\n",
    "- The confusion matrix shows which classes are correctly/incorrectly predicted. Diagonal cells are correct predictions; off-diagonals indicate misclassifications.\n",
    "- The plotted tree shows:\n",
    "  - Each node’s splitting rule (feature and threshold)\n",
    "  - Samples, class distribution, and Gini impurity at the node\n",
    "  - Leaf nodes’ predicted class (color-coded)\n",
    "- Try adjusting `max_depth`, `min_samples_split`, or `criterion='entropy'` to see how complexity impacts accuracy and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression in Python (Diabetes)\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load regression dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "feature_names = load_diabetes().feature_names\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Model: limit depth to reduce overfitting\n",
    "regr = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = regr.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R^2: {r2:.3f}\")\n",
    "\n",
    "# Feature importance plot\n",
    "importances = regr.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xticks(range(len(importances)), np.array(feature_names)[indices], rotation=45, ha='right')\n",
    "plt.title('Feature Importances - Decision Tree Regressor (Diabetes)')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Predicted vs True\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "plt.plot(lims, lims, 'r--', label='Ideal')\n",
    "plt.xlabel('True'); plt.ylabel('Predicted')\n",
    "plt.legend(); plt.title('True vs Predicted - Decision Tree Regressor')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c59d5a",
   "metadata": {},
   "source": [
    "### Interpreting the Decision Tree Regression Output\n",
    "- We fit a `DecisionTreeRegressor` with a limited `max_depth` to control variance.\n",
    "- RMSE quantifies average prediction error; lower is better. R^2 indicates the fraction of variance explained by the model.\n",
    "- The feature importance bar chart shows which features the tree used most in splits.\n",
    "- The True vs Predicted scatter should cluster near the red dashed identity line; deviations indicate error.\n",
    "- Try tuning `max_depth`, `min_samples_leaf`, and using cross-validation to balance bias-variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
